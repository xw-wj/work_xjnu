import pandas as pd
import json
from pathlib import Path

# è¯»å–åŸå§‹æ•°æ®
input_file = "dataset/data/train-00000-of-00001-ebfa7c1c3a087835.parquet"
df = pd.read_parquet(input_file)

print(f"åŸå§‹æ•°æ®é›†å¤§å°: {len(df)} æ¡è®°å½•")
print("=" * 80)

# ============================================================================
# æ–¹æ¡ˆ1: è½¬æ¢ä¸º ShareGPT æ ¼å¼ï¼ˆæ¨èï¼Œæ”¯æŒå¤šè½®å¯¹è¯ï¼‰
# ============================================================================
def convert_to_sharegpt():
    """
    è½¬æ¢ä¸º ShareGPT æ ¼å¼çš„å¤šè½®å¯¹è¯æ•°æ®
    æ¯ä¸ª utterance åŒ…å«å¤šè½®å¯¹è¯ï¼ŒæŒ‰é¡ºåºäº¤æ›¿åˆ†é…ç»™ human å’Œ gpt
    """
    sharegpt_data = []
    
    for idx, row in df.iterrows():
        utterance = row['utterance']
        
        # è·³è¿‡æ— æ•ˆæ•°æ®
        if not isinstance(utterance, dict) or 'lines' not in utterance:
            continue
        
        lines = utterance['lines']
        
        # è‡³å°‘éœ€è¦2æ¡å¯¹è¯ï¼ˆä¸€é—®ä¸€ç­”ï¼‰
        if len(lines) < 2:
            continue
        
        # æ„å»ºå¯¹è¯åˆ—è¡¨
        conversations = []
        for i, line in enumerate(lines):
            # æ¸…ç†æ–‡æœ¬ï¼šå»é™¤äººç‰©åç§°æ ‡ç­¾ï¼ˆå¦‚ "BIANCA\n"ï¼‰
            text = line.strip()
            
            # æ–¹æ³•1: å»é™¤å¼€å¤´çš„å¤§å†™äººåï¼ˆåŒ¹é…æ¨¡å¼ï¼šå…¨å¤§å†™å•è¯ + æ¢è¡Œï¼‰
            import re
            text = re.sub(r'^[A-Z][A-Z\s]+\n', '', text)
            
            # æ–¹æ³•2: å¦‚æœäººååé¢æœ‰æ¢è¡Œï¼Œå»é™¤ç¬¬ä¸€è¡Œ
            # if '\n' in text:
            #     lines_split = text.split('\n', 1)
            #     if lines_split[0].isupper():  # ç¬¬ä¸€è¡Œå…¨å¤§å†™
            #         text = lines_split[1] if len(lines_split) > 1 else text
            
            # å†æ¬¡æ¸…ç†å¤šä½™ç©ºç™½
            text = text.strip()
            
            # è·³è¿‡ç©ºæ–‡æœ¬
            if not text:
                continue
            
            # å¥‡æ•°ä½ç½®æ˜¯ humanï¼Œå¶æ•°ä½ç½®æ˜¯ gpt
            role = "human" if i % 2 == 0 else "gpt"
            
            conversations.append({
                "from": role,
                "value": text
            })
        
        # ============================================================
        # ç³»ç»Ÿæç¤ºè¯é€‰é¡¹ï¼ˆæ ¹æ®éœ€æ±‚é€‰æ‹©ï¼‰
        # ============================================================
        
        # é€‰é¡¹1: ä¸ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯ï¼ˆæ¨è - è®­ç»ƒé€šç”¨å¯¹è¯æ¨¡å‹ï¼‰
        sharegpt_data.append({
            "conversations": conversations
        })
        
        # é€‰é¡¹2: ä½¿ç”¨ç”µå½±ä¿¡æ¯ä½œä¸ºèƒŒæ™¯ï¼ˆè®­ç»ƒç”µå½±å¯¹è¯é£æ ¼æ¨¡å‹ï¼‰
        # system_prompt = f"è¿™æ˜¯ä¸€æ®µæ¥è‡ªç”µå½±ã€Š{row['movieTitle'].strip()}ã€‹({row['movieYear'].strip()})çš„å¯¹è¯ã€‚"
        # sharegpt_data.append({
        #     "conversations": conversations,
        #     "system": system_prompt
        # })
        
        # é€‰é¡¹3: ä½¿ç”¨é€šç”¨å¯¹è¯ç³»ç»Ÿæç¤ºè¯
        # sharegpt_data.append({
        #     "conversations": conversations,
        #     "system": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€è‡ªç„¶çš„å¯¹è¯åŠ©æ‰‹ã€‚"
        # })
    
    return sharegpt_data

# ============================================================================
# æ–¹æ¡ˆ2: è½¬æ¢ä¸º Alpaca æ ¼å¼ï¼ˆå•è½®å¯¹è¯ï¼‰
# ============================================================================
def convert_to_alpaca():
    """
    è½¬æ¢ä¸º Alpaca æ ¼å¼
    å°†æ¯ä¸ª utterance çš„ç¬¬ä¸€å¥ä½œä¸º instructionï¼Œæœ€åä¸€å¥ä½œä¸º output
    """
    alpaca_data = []
    
    for idx, row in df.iterrows():
        utterance = row['utterance']
        
        if not isinstance(utterance, dict) or 'lines' not in utterance:
            continue
        
        lines = utterance['lines']
        
        if len(lines) < 2:
            continue
        
        # ç¬¬ä¸€å¥ä½œä¸ºäººç±»æŒ‡ä»¤
        instruction = lines[0].strip()
        
        # æœ€åä¸€å¥ä½œä¸ºæ¨¡å‹å›ç­”
        output = lines[-1].strip()
        
        # ä¸­é—´çš„å¯¹è¯ä½œä¸ºå†å²è®°å½•
        history = []
        for i in range(1, len(lines) - 1, 2):
            if i + 1 < len(lines):
                history.append([
                    lines[i].strip(),
                    lines[i + 1].strip()
                ])
        
        alpaca_data.append({
            "instruction": instruction,
            "input": "",
            "output": output,
            "system": f"è¿™æ˜¯ä¸€æ®µæ¥è‡ªç”µå½±ã€Š{row['movieTitle'].strip()}ã€‹çš„å¯¹è¯ã€‚",
            "history": history if history else []
        })
    
    return alpaca_data

# ============================================================================
# æ‰§è¡Œè½¬æ¢
# ============================================================================
print("\né€‰æ‹©è½¬æ¢æ ¼å¼ï¼š")
print("1. ShareGPT æ ¼å¼ï¼ˆæ¨èï¼Œå®Œæ•´ä¿ç•™å¤šè½®å¯¹è¯ï¼‰")
print("2. Alpaca æ ¼å¼ï¼ˆæå–é¦–å°¾å¯¹è¯ï¼Œä¸­é—´ä½œä¸ºå†å²ï¼‰")
print()

# é»˜è®¤ä½¿ç”¨ ShareGPT æ ¼å¼
use_sharegpt = True

if use_sharegpt:
    print("âœ“ ä½¿ç”¨ ShareGPT æ ¼å¼è½¬æ¢...")
    converted_data = convert_to_sharegpt()
    output_file = "dataset/data/train_sharegpt.json"
    format_name = "sharegpt"
else:
    print("âœ“ ä½¿ç”¨ Alpaca æ ¼å¼è½¬æ¢...")
    converted_data = convert_to_alpaca()
    output_file = "dataset/data/train_alpaca.json"
    format_name = "alpaca"

# ä¿å­˜è½¬æ¢åçš„æ•°æ®
Path(output_file).parent.mkdir(parents=True, exist_ok=True)
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(converted_data, f, ensure_ascii=False, indent=2)

print(f"\nâœ… è½¬æ¢å®Œæˆï¼")
print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")
print(f"   è½¬æ¢åæ•°æ®é‡: {len(converted_data)} æ¡")
print(f"   åŸå§‹æ•°æ®é‡: {len(df)} æ¡")
print()

# ============================================================================
# ç”Ÿæˆ dataset_info.json é…ç½®
# ============================================================================
dataset_info = {}

if use_sharegpt:
    dataset_info["movie_dialogue_sharegpt"] = {
        "file_name": "train_sharegpt.json",
        "formatting": "sharegpt",
        "columns": {
            "messages": "conversations"
            # å¦‚æœä½¿ç”¨äº† system å­—æ®µï¼Œå–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Šï¼š
            # "system": "system"
        }
    }
else:
    dataset_info["movie_dialogue_alpaca"] = {
        "file_name": "train_alpaca.json",
        "columns": {
            "prompt": "instruction",
            "query": "input",
            "response": "output",
            "system": "system",
            "history": "history"
        }
    }

# ä¿å­˜ dataset_info.json
dataset_info_file = "dataset/dataset_info.json"
with open(dataset_info_file, 'w', encoding='utf-8') as f:
    json.dump(dataset_info, f, ensure_ascii=False, indent=2)

print(f"ğŸ“‹ å·²ç”Ÿæˆ dataset_info.json é…ç½®æ–‡ä»¶: {dataset_info_file}")
print()
print("=" * 80)
print("ğŸ“– ä½¿ç”¨è¯´æ˜ï¼š")
print("=" * 80)
print(f"1. å°†ç”Ÿæˆçš„æ–‡ä»¶å¤åˆ¶åˆ° LLaMA Factory çš„ data ç›®å½•")
print(f"2. å°† dataset_info.json çš„å†…å®¹æ·»åŠ åˆ° LLaMA Factory çš„ data/dataset_info.json")
print(f"3. åœ¨è®­ç»ƒæ—¶ä½¿ç”¨æ•°æ®é›†åç§°: movie_dialogue_{format_name}")
print()

# æ˜¾ç¤ºå‰3æ¡è½¬æ¢åçš„æ•°æ®æ ·ä¾‹
print("=" * 80)
print("ğŸ“ è½¬æ¢åçš„æ•°æ®æ ·ä¾‹ï¼ˆå‰3æ¡ï¼‰ï¼š")
print("=" * 80)
for i, item in enumerate(converted_data[:3], 1):
    print(f"\nç¬¬ {i} æ¡æ•°æ®ï¼š")
    print(json.dumps(item, ensure_ascii=False, indent=2))
    print("-" * 80)